{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819d984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2a32b",
   "metadata": {},
   "source": [
    "### 학습데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3dad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/train/가볍다/1.mp4</td>\n",
       "      <td>가볍다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/train/가져오다/2.mp4</td>\n",
       "      <td>가져오다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/train/가짜/3.mp4</td>\n",
       "      <td>가짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/train/가치/4.mp4</td>\n",
       "      <td>가치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/train/보관/5.mp4</td>\n",
       "      <td>보관</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset/train/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset/train/보다/7.mp4</td>\n",
       "      <td>보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset/train/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset/train/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 video_name   tag\n",
       "0   dataset/train/가볍다/1.mp4   가볍다\n",
       "1  dataset/train/가져오다/2.mp4  가져오다\n",
       "2    dataset/train/가짜/3.mp4    가짜\n",
       "3    dataset/train/가치/4.mp4    가치\n",
       "4    dataset/train/보관/5.mp4    보관\n",
       "5   dataset/train/보내다/6.mp4   보내다\n",
       "6    dataset/train/보다/7.mp4    보다\n",
       "7    dataset/train/안경/8.mp4    안경\n",
       "8    dataset/train/알다/9.mp4    알다\n",
       "9  dataset/train/월요일/10.mp4   월요일"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/train')\n",
    "label_types = os.listdir('dataset/train')\n",
    "# 훈련 데이터셋을 위한 비어있는 리스트 초기화\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/train' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/train'+'/'+item)    \n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/train'+'/'+item)+'/'+room))\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name']).loc[:,['video_name','tag']]\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('train.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5315f",
   "metadata": {},
   "source": [
    "### 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76e8eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/test/가볍다/1.mp4</td>\n",
       "      <td>가볍다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/test/가져오다/2.mp4</td>\n",
       "      <td>가져오다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/test/가짜/3.mp4</td>\n",
       "      <td>가짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/test/가치/4.mp4</td>\n",
       "      <td>가치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/test/보관/5.mp4</td>\n",
       "      <td>보관</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset/test/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset/test/보다/7.mp4</td>\n",
       "      <td>보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset/test/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset/test/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset/test/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                video_name   tag\n",
       "0   dataset/test/가볍다/1.mp4   가볍다\n",
       "1  dataset/test/가져오다/2.mp4  가져오다\n",
       "2    dataset/test/가짜/3.mp4    가짜\n",
       "3    dataset/test/가치/4.mp4    가치\n",
       "4    dataset/test/보관/5.mp4    보관\n",
       "5   dataset/test/보내다/6.mp4   보내다\n",
       "6    dataset/test/보다/7.mp4    보다\n",
       "7    dataset/test/안경/8.mp4    안경\n",
       "8    dataset/test/알다/9.mp4    알다\n",
       "9  dataset/test/월요일/10.mp4   월요일"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "room_types = os.listdir('dataset/test')\n",
    "\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/test' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/test'+'/'+item)\n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/test'+'/'+item)+'/'+room))\n",
    "\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name'])\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('test.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c02f2a",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65843333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video for training: 10\n",
      "Total video for testing: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dataset/train/가볍다/1.mp4</td>\n",
       "      <td>가볍다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dataset/train/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dataset/train/가짜/3.mp4</td>\n",
       "      <td>가짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dataset/train/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                video_name  tag\n",
       "0           0   dataset/train/가볍다/1.mp4  가볍다\n",
       "7           7    dataset/train/안경/8.mp4   안경\n",
       "8           8    dataset/train/알다/9.mp4   알다\n",
       "2           2    dataset/train/가짜/3.mp4   가짜\n",
       "9           9  dataset/train/월요일/10.mp4  월요일"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋과 테스트 데이터셋을 각각 CSV 파일에서 로드\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 훈련 및 테스트 데이터셋 크기 출력\n",
    "print(f\"Total video for training: {len(train_df)}\")\n",
    "print(f\"Total video for testing: {len(test_df)}\")\n",
    "\n",
    "# 훈련 데이터셋의 샘플 5개 출력\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06a00c",
   "metadata": {},
   "source": [
    "### Feed the video to a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e11d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f7c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지에서 중앙에 맞춰 정사각형으로 잘나내는 함수\n",
    "def crop_center_square(frame):\n",
    "    # 이미지의 높이(y)와 너비(x)를 가져옴\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 이미지의 높이와 너비 중 더 작은 값을 선택하여 정사각형의 크기를 결정\n",
    "    min_dim = min(y, x)\n",
    "    # 정사각형을 이미지 중앙에 위치시키기 위해 시작점의 x좌표와 y좌표를 계산\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 계산된 시작점과 정사각형의 크기를 이용하여 이미지의 중앙 부분을 잘라냅니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "# 비디오 파일을 로드하고, 각 프레임을 처리하여 배열로 반환하는 함수\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    # OpenCV를 사용하여 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    skeletons = []  # 스켈레톤 데이터\n",
    "    hand_landmarks = []  # 손 데이터\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 비디오에서 프레임을 하나씩 읽기\n",
    "            ret, frame = cap.read()\n",
    "            # 읽을 프레임이 없으면 반복문을 종료\n",
    "            if not ret:\n",
    "                break\n",
    "            # 읽은 프레임에서 중앙의 정사각형 부분을 잘라냄\n",
    "            frame = crop_center_square(frame)\n",
    "            # 프레임의 크기를 지정된 크기로 조절\n",
    "            frame = cv2.resize(frame, resize)            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Mediapipe를 사용하여 스켈레톤 추출\n",
    "            hands_results = hands.process(frame_rgb)\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "\n",
    "           \n",
    "            if pose_results.pose_landmarks:\n",
    "                skeletons.append(pose_results.pose_landmarks.landmark)\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                hand_landmarks.append(hands_results.multi_hand_landmarks)\n",
    "            \n",
    "            # OpenCV는 BGR 색상 순서를 사용하므로, 이를 RGB 순서로 변경\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            # 처리된 프레임을 프레임 리스트에 추가\n",
    "            frames.append(frame)\n",
    "            # max_frames가 지정된 경우, 지정된 수의 프레임만큼만 처리\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        # 비디오 파일을 닫기\n",
    "        cap.release()\n",
    "        pose.close\n",
    "        hands.close\n",
    "\n",
    "    # 처리된 모든 프레임을 numpy 배열로 변환하여 반환\n",
    "    return np.array(frames), skeletons, hand_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71374a5e",
   "metadata": {},
   "source": [
    "### 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c477f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    # 이미지 특징 추출을 위한 InceptionV3 모델\n",
    "    base_model = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    image_input = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed_image = preprocess_input(image_input)\n",
    "    image_features = base_model(preprocessed_image)\n",
    "\n",
    "    # Mediapipe 데이터를 위한 입력 레이어 및 처리 레이어\n",
    "    # 예시로, Mediapipe 데이터의 차원을 상정하여 입력 레이어를 정의\n",
    "    mediapipe_input = keras.Input((258,))\n",
    "    mediapipe_features = keras.layers.Dense(128, activation=\"relu\")(mediapipe_input)\n",
    "\n",
    "    # 이미지 특징과 Mediapipe 데이터의 결합\n",
    "    combined_features = keras.layers.concatenate([image_features, mediapipe_features])\n",
    "\n",
    "    # 최종 모델\n",
    "    outputs = keras.layers.Dense(10, activation=\"softmax\")(combined_features)\n",
    "    return keras.Model(inputs=[image_input, mediapipe_input], outputs=outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477297ac",
   "metadata": {},
   "source": [
    "### label 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241c78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "['가볍다', '가져오다', '가짜', '가치', '보관', '보내다', '보다', '안경', '알다', '월요일']\n",
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[...,None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7078c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "SKELETON_FEATURES = 33*4\n",
    "HAND_FEATURES = 21*3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b94b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths =train_df[\"video_name\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e7cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKELETON_FEATURES = 33*4\n",
    "HAND_FEATURES = 21*3*2\n",
    "def preprocess_skeleton_data(skeleton):\n",
    "    # 스켈레톤 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not skeleton:\n",
    "        return np.zeros(SKELETON_FEATURES)\n",
    "\n",
    "    # 스켈레톤 데이터를 1차원 배열로 변환\n",
    "    skeleton_array = np.array([[lm.x, lm.y, lm.z] for lm in skeleton]).flatten()\n",
    "    return skeleton_array\n",
    "\n",
    "def preprocess_hand_data(hand_landmarks):\n",
    "    # 손 랜드마크 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not hand_landmarks:\n",
    "        return np.zeros(HAND_FEATURES)\n",
    "\n",
    "    # 모든 손 랜드마크를 하나의 벡터로 결합\n",
    "    hand_data = []\n",
    "    for hand_lm in hand_landmarks:\n",
    "        # 각 손의 랜드마크를 1차원 배열로 변환\n",
    "        lm_array = np.array([[lm.x, lm.y, lm.z] for lm in hand_lm.landmark]).flatten()\n",
    "        hand_data.extend(lm_array)\n",
    "\n",
    "    return np.array(hand_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047da826",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1, 162\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (frame_features, frame_skeletons, frame_hands, frame_masks), labels\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 사용 예시\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m train_data, train_labels \u001b[38;5;241m=\u001b[39m prepare_all_video(train_df)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_labels)\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36mprepare_all_video\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     27\u001b[0m combined_mediapipe_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([skeleton_feature, hand_feature])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 이미지 프레임 특징 추출\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m frame_feature \u001b[38;5;241m=\u001b[39m feature_extractor_model\u001b[38;5;241m.\u001b[39mpredict([frames[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], combined_mediapipe_data])                                      \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 특징 저장\u001b[39;00m\n\u001b[0;32m     34\u001b[0m frame_features[idx, i, :] \u001b[38;5;241m=\u001b[39m frame_feature\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1960\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1953\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1954\u001b[0m         label,\n\u001b[0;32m   1955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1956\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1957\u001b[0m         ),\n\u001b[0;32m   1958\u001b[0m     )\n\u001b[0;32m   1959\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1960\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 162\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "def prepare_all_video(df):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # 수정: Mediapipe 데이터를 저장할 배열 초기화\n",
    "    frame_skeletons = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    \n",
    "    # 특징 추출기 모델 초기화\n",
    "    feature_extractor_model = build_feature_extractor()\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames, skeletons, hands = load_video(os.path.join(path))\n",
    "        video_length = min(MAX_SEQ_LENGTH, len(frames))\n",
    "\n",
    "        for i in range(video_length):           \n",
    "\n",
    "            # Mediapipe 데이터 전처리\n",
    "            skeleton_feature = preprocess_skeleton_data(skeletons[i])\n",
    "            hand_feature = preprocess_hand_data(hands[i])\n",
    "\n",
    "            # 데이터 결합 및 배치 차원 추가\n",
    "            combined_mediapipe_data = np.concatenate([skeleton_feature, hand_feature])\n",
    "\n",
    "            # 이미지 프레임 특징 추출\n",
    "            frame_feature = feature_extractor_model.predict([frames[i:i+1], combined_mediapipe_data])                                      \n",
    "                                                     \n",
    "                                                     \n",
    "            # 특징 저장\n",
    "            frame_features[idx, i, :] = frame_feature\n",
    "            frame_skeletons[idx, i, :] = skeleton_feature\n",
    "            frame_hands[idx, i, :] = hand_feature\n",
    "            frame_masks[idx, i] = 1\n",
    "\n",
    "    # 수정: 반환 값에 Mediapipe 데이터 포함\n",
    "    return (frame_features, frame_skeletons, frame_hands, frame_masks), labels\n",
    "\n",
    "# 사용 예시\n",
    "train_data, train_labels = prepare_all_video(train_df)\n",
    "print(train_data)\n",
    "print(train_labels)\n",
    "test_data, test_labels = prepare_all_video(test_df)\n",
    "train_labels = np.squeeze(train_labels)\n",
    "test_labels = np.squeeze(test_labels)\n",
    "\n",
    "print(f\"Frame feature in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "print(f\"train_labels in train set:{train_labels.shape}\")\n",
    "print(f\"test_labels in train set:{test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b7a22",
   "metadata": {},
   "source": [
    "### sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78983480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    # 기존 이미지 특징에 대한 입력\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    # Mediapipe 데이터에 대한 추가 입력 레이어\n",
    "    skeleton_input = keras.Input((MAX_SEQ_LENGTH, SKELETON_FEATURES))\n",
    "    hand_input = keras.Input((MAX_SEQ_LENGTH, HAND_FEATURES))\n",
    "    \n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    \n",
    "    # 이미지 특징 처리를 위한 GRU 레이어\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8, return_sequences=True)(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)  # 형태 변경\n",
    "    \n",
    "    # Mediapipe 데이터를 처리하는 추가 네트워크 레이어 (예시)\n",
    "    # 여기서는 간단히 Dense 레이어를 사용했지만, 필요에 따라 다른 구조를 사용할 수 있습니다.\n",
    "    y_skeleton = keras.layers.GlobalAveragePooling1D()(skeleton_input)\n",
    "    y_hand = keras.layers.GlobalAveragePooling1D()(hand_input)\n",
    "\n",
    "\n",
    "    # 모든 특징을 결합\n",
    "    combined = keras.layers.concatenate([x, y_skeleton, y_hand])\n",
    "\n",
    "    # 결합된 특징에 대한 추가 처리\n",
    "    z = keras.layers.Dense(16, activation=\"relu\")(combined)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(z)\n",
    "    \n",
    "    rnn_model = keras.Model([frame_features_input, skeleton_input, hand_input, mask_input], output)\n",
    "    rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1], train_data[2], train_data[3], train_data[4]],  # 수정된 입력 데이터\n",
    "        train_labels,\n",
    "        batch_size=1,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate(\n",
    "        [test_data[0], test_data[1], test_data[2], test_data[3], test_data[4]],  # 수정된 입력 데이터\n",
    "        test_labels\n",
    "    )\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eadeba6",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90405fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames, skeletons, hands):\n",
    "    frames = frames[None,...]\n",
    "    frame_mask = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Mediapipe 데이터를 위한 배열 초기화\n",
    "    frame_skeletons = np.zeros((1, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros((1, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    video_length = min(MAX_SEQ_LENGTH, frames.shape[1])\n",
    "\n",
    "    for j in range(video_length):\n",
    "        frame_features[0, j, :] = feature_extractor.predict(frames[0, j, :])\n",
    "\n",
    "        # Mediapipe 데이터 처리 및 저장\n",
    "        frame_skeletons[0, j, :] = preprocess_skeleton_data(skeletons[j])\n",
    "        frame_hands[0, j, :] = preprocess_hand_data(hands[j])\n",
    "\n",
    "        frame_mask[0, j] = 1\n",
    "\n",
    "    return frame_features, frame_skeletons, frame_hands, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    frames, skeletons, hands = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_skeletons, frame_hands, frame_mask = prepare_single_video(frames, skeletons, hands)\n",
    "\n",
    "    probabilities = sequence_model.predict([frame_features, frame_skeletons, frame_hands,frame_mask])[0]\n",
    "    \n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"{class_vocab[i]} : {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "    \n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path : {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
