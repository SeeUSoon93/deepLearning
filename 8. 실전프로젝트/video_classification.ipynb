{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6e5685",
   "metadata": {},
   "source": [
    "### 제목: CNN-RNN 구조를 이용한 비디오 분류\n",
    "\n",
    "#### 작성자: 사야크 폴\n",
    "\n",
    "#### 생성 날짜: 2021/05/28\n",
    "\n",
    "#### 마지막 수정: 2023/08/28\n",
    "\n",
    "#### 설명: UCF101 데이터셋에서 전이 학습과 순환 모델을 이용한 비디오 분류기 훈련.\n",
    "\n",
    "#### 가속기: GPU\n",
    "\n",
    "- 이 예제는 추천, 보안 등에 응용 가능한 중요한 사용 사례인 비디오 분류를 보여줍니다. 우리는 UCF101 데이터셋을 사용하여 비디오 분류기를 구축할 것입니다. 이 데이터셋은 크리켓 샷, 펀칭, 바이킹 등 다양한 행동으로 분류된 비디오들로 구성되어 있습니다. 이 데이터셋은 행동 인식기를 구축하는 데 자주 사용되며, 비디오 분류의 응용 프로그램입니다.\n",
    "\n",
    "- 비디오는 순서가 있는 프레임 시퀀스로 구성됩니다. 각 프레임은 공간적 정보를 포함하며, 이러한 프레임의 시퀀스는 시간적 정보를 포함합니다. 이 두 가지 측면을 모두 모델링하기 위해, 우리는 공간 처리를 위한 컨볼루션(합성곱)과 시간 처리를 위한 순환 레이어를 사용하는 혼합 구조를 사용합니다. 구체적으로, 우리는 GRU 레이어가 있는 컨볼루셔널 신경망(CNN)과 순환 신경망(RNN)을 사용할 것입니다. 이런 형태의 혼합 구조는 흔히 CNN-RNN으로 알려져 있습니다.\n",
    "\n",
    "- 이 예제는 TensorFlow 2.5 이상, 그리고 TensorFlow Docs가 필요하며, 다음 명령어를 사용하여 설치할 수 있습니다:\n",
    "\n",
    "pip install -q git+https://github.com/tensorflow/docs\n",
    "\n",
    "#### 데이터 수집\n",
    "- 이 예제의 실행 시간을 상대적으로 짧게 유지하기 위해, 원본 UCF101 데이터셋의 하위 샘플 버전을 사용할 것입니다. 샘플링이 어떻게 이루어졌는지에 대해서는 이 노트북을 참조하십시오.\n",
    "\n",
    "!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
    "tar xf ucf101_top5.tar.gz\n",
    "\n",
    "#### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa7291a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m## 데이터 준비 부분\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 훈련 및 테스트 데이터를 포함하는 CSV 파일을 읽어서 DataFrame 객체를 생성합니다.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 훈련 데이터셋을 불러옵니다.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 테스트 데이터셋을 불러옵니다.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 훈련 및 테스트 데이터셋의 비디오 수를 출력합니다.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리들을 임포트합니다.\n",
    "from tensorflow_docs.vis import embed  # TensorFlow 문서의 시각화를 위한 도구입니다.\n",
    "from tensorflow import keras  # TensorFlow의 고수준 신경망 API입니다.\n",
    "from imutils import paths  # 이미지 처리를 위한 유틸리티 함수들을 제공합니다.\n",
    "\n",
    "import matplotlib.pyplot as plt  # 그래프와 이미지를 시각화하기 위한 라이브러리입니다.\n",
    "import tensorflow as tf  # TensorFlow 라이브러리입니다, 딥러닝 모델을 구성하고 훈련하기 위해 사용됩니다.\n",
    "import pandas as pd  # 데이터 분석 및 조작을 위한 라이브러리입니다.\n",
    "import numpy as np  # 수치 계산을 위한 라이브러리입니다.\n",
    "import imageio  # 이미지 읽기/쓰기를 위한 라이브러리입니다.\n",
    "import cv2  # OpenCV 라이브러리, 이미지 및 비디오 처리를 위해 사용됩니다.\n",
    "import os  # 운영체제와 상호작용을 위한 라이브러리입니다, 파일 경로 조작에 주로 사용됩니다.\n",
    "\n",
    "\"\"\"\n",
    "## 하이퍼파라미터 정의 부분\n",
    "\"\"\"\n",
    "\n",
    "# 이미지의 크기, 배치 크기, 에포크 수를 정의하는 하이퍼파라미터입니다.\n",
    "IMG_SIZE = 224  # 입력 이미지의 크기를 정의합니다.\n",
    "BATCH_SIZE = 64  # 한 번에 처리할 이미지의 수를 정의합니다.\n",
    "EPOCHS = 10  # 모델을 훈련할 때 전체 데이터셋을 반복할 횟수를 정의합니다.\n",
    "\n",
    "# 비디오 처리에 사용할 최대 시퀀스 길이와 특징 벡터의 크기를 정의합니다.\n",
    "MAX_SEQ_LENGTH = 20  # 처리할 비디오의 최대 프레임 수를 정의합니다.\n",
    "NUM_FEATURES = 2048  # 비디오 프레임에서 추출할 특징의 차원 수를 정의합니다.\n",
    "\n",
    "\"\"\"\n",
    "## 데이터 준비 부분\n",
    "\"\"\"\n",
    "\n",
    "# 훈련 및 테스트 데이터를 포함하는 CSV 파일을 읽어서 DataFrame 객체를 생성합니다.\n",
    "train_df = pd.read_csv(\"train.csv\")  # 훈련 데이터셋을 불러옵니다.\n",
    "test_df = pd.read_csv(\"test.csv\")  # 테스트 데이터셋을 불러옵니다.\n",
    "\n",
    "# 훈련 및 테스트 데이터셋의 비디오 수를 출력합니다.\n",
    "print(f\"Total videos for training: {len(train_df)}\")  # 훈련용 비디오의 총 개수를 출력합니다.\n",
    "print(f\"Total videos for testing: {len(test_df)}\")  # 테스트용 비디오의 총 개수를 출력합니다.\n",
    "\n",
    "# 훈련 데이터셋에서 무작위로 10개의 샘플을 출력합니다.\n",
    "train_df.sample(10)  # 훈련 데이터셋의 예시를 출력하기 위해 무작위로 10개의 샘플을 선택합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "비디오 분류기를 훈련하는 많은 도전 과제 중 하나는 네트워크에 비디오를 어떻게 공급할지를 결정하는 것입니다. \n",
    "특정 블로그 포스트에서는 이러한 방법 다섯 가지를 논의합니다. 비디오는 순서가 있는 프레임의 시퀀스이므로, \n",
    "우리는 단순히 프레임들을 추출하여 3D 텐서로 만들 수 있습니다. 그러나 비디오마다 프레임 수가 다를 수 있으므로 \n",
    "패딩(padding)을 사용하지 않는 한 이들을 배치로 쌓는 것이 불가능합니다. 대안으로, 우리는 최대 프레임 수에 도달할 때까지 \n",
    "고정 간격으로 비디오 프레임을 저장할 수 있습니다. 이 예제에서는 다음과 같은 작업을 수행할 것입니다:\n",
    "\n",
    "1. 비디오의 프레임을 캡처합니다.\n",
    "2. 최대 프레임 수에 도달할 때까지 비디오에서 프레임을 추출합니다.\n",
    "3. 비디오의 프레임 수가 최대 프레임 수보다 적은 경우, 우리는 비디오를 0으로 채우는 패딩 작업을 할 것입니다.\n",
    "\n",
    "이 작업 흐름은 텍스트 시퀀스를 다루는 문제와 동일함을 주목해야 합니다. UCF101 데이터셋의 비디오는 프레임 간에 객체와 동작의 \n",
    "극단적인 변화가 없다고 알려져 있습니다. 이 때문에, 학습 작업을 위해 몇몇 프레임만을 고려하는 것이 괜찮을 수 있습니다. \n",
    "그러나 이 접근법은 다른 비디오 분류 문제에 잘 일반화되지 않을 수 있습니다. 우리는 OpenCV의 `VideoCapture()` 메소드를 사용하여 \n",
    "비디오에서 프레임을 읽을 것입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 두 메서드는 이 튜토리얼에서 가져왔습니다:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "\n",
    "# 비디오 프레임의 중앙을 정사각형으로 자르는 함수입니다.\n",
    "def crop_center_square(frame):\n",
    "    # 프레임의 높이와 너비를 가져옵니다.\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 높이와 너비 중 더 작은 값을 선택합니다.\n",
    "    min_dim = min(y, x)\n",
    "    # 중앙에서 시작할 x, y 좌표를 계산합니다.\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 정사각형 프레임을 반환합니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "# 비디오를 로드하고, 선택적으로 프레임의 수를 제한하며, 프레임을 재조정하는 함수입니다.\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    # 비디오 경로로부터 비디오 캡처 객체를 생성합니다.\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []  # 프레임을 저장할 리스트입니다.\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # 프레임을 하나씩 읽습니다.\n",
    "            if not ret:\n",
    "                break  # 더 이상 프레임이 없으면 종료합니다.\n",
    "            frame = crop_center_square(frame)  # 중앙 정사각형으로 자릅니다.\n",
    "            frame = cv2.resize(frame, resize)  # 프레임을 재조정합니다.\n",
    "            frame = frame[:, :, [2, 1, 0]]  # BGR을 RGB로 순서를 변경합니다.\n",
    "            frames.append(frame)  # 프레임을 리스트에 추가합니다.\n",
    "\n",
    "            if len(frames) == max_frames:  # 최대 프레임 수에 도달하면 종료합니다.\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()  # 비디오 캡처 객체를 해제합니다.\n",
    "    return np.array(frames)  # 프레임의 배열을 반환합니다.\n",
    "\n",
    "\"\"\"\n",
    "# 사전 훈련된 네트워크를 사용하여 추출된 프레임에서 의미 있는 특징을 추출할 수 있습니다.\n",
    "# [`Keras Applications`](https://keras.io/api/applications/) 모듈은\n",
    "# [ImageNet-1k 데이터셋](http://image-net.org/)에서 사전 훈련된 다양한 최신 모델들을 제공합니다.\n",
    "# 우리는 이 목적으로 [InceptionV3 모델](https://arxiv.org/abs/1512.00567)을 사용할 것입니다.\n",
    "\"\"\"\n",
    "\n",
    "# 특징 추출기를 구축하는 함수입니다.\n",
    "def build_feature_extractor():\n",
    "    # InceptionV3 모델을 특징 추출기로 사용합니다. ImageNet 데이터로 사전 훈련된 가중치를 사용하고, \n",
    "    # 최상위 층은 포함하지 않으며, 평균 풀링을 사용합니다.\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    # InceptionV3에 맞게 입력 데이터를 전처리하는 함수입니다.\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # 모델의 입력을 정의합니다.\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    # 입력 데이터를 전처리합니다.\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    # 전처리된 데이터를 특징 추출기에 통과시켜 출력을 얻습니다.\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    # 입력과 출력을 연결하는 케라스 모델을 생성합니다.\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "# 특징 추출기 모델을 생성합니다.\n",
    "feature_extractor = build_feature_extractor()\n",
    "\n",
    "\"\"\"\n",
    "# 비디오의 라벨은 문자열입니다. 신경망은 문자열 값을 이해할 수 없으므로,\n",
    "# 모델에 제공되기 전에 일종의 수치 형태로 변환되어야 합니다. 여기서는\n",
    "# [`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\n",
    "# 레이어를 사용하여 클래스 라벨을 정수로 인코딩합니다.\n",
    "\"\"\"\n",
    "\n",
    "# 클래스 라벨을 정수로 변환하는 레이어를 생성합니다.\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "# 라벨 프로세서가 이해하는 어휘목록을 출력합니다.\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 마지막으로, 모든 부분을 함께 모아서 데이터 처리 유틸리티를 만듭니다.\n",
    "\"\"\"\n",
    "\n",
    "# 주어진 데이터프레임(df)과 루트 디렉터리(root_dir)를 사용하여 모든 비디오를 준비하는 함수입니다.\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)  # 샘플의 수를 결정합니다.\n",
    "    video_paths = df[\"video_name\"].values.tolist()  # 비디오 경로를 리스트로 변환합니다.\n",
    "    labels = df[\"tag\"].values  # 라벨 값을 가져옵니다.\n",
    "    labels = label_processor(labels[..., None]).numpy()  # 라벨을 처리하여 넘파이 배열로 변환합니다.\n",
    "\n",
    "    # `frame_masks`와 `frame_features`는 우리의 순차 모델에 제공될 데이터입니다.\n",
    "    # `frame_masks`는 시간 단계가 패딩으로 마스킹되었는지 여부를 나타내는 부울 값들을 포함합니다.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # 각 비디오에 대해서.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # 모든 프레임을 수집하고 배치 차원을 추가합니다.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # 현재 비디오의 마스크와 특징을 저장하기 위한 임시 공간을 초기화합니다.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # 현재 비디오의 프레임에서 특징을 추출합니다.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                # 특징 추출기를 사용하여 각 프레임의 특징을 예측합니다.\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            # 마스크를 설정합니다. 1 = 마스킹되지 않음, 0 = 마스킹됨\n",
    "            temp_frame_mask[i, :length] = 1  \n",
    "\n",
    "        # 임시 특징과 마스크를 각각의 특징 및 마스크 배열에 할당합니다.\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    # 특징과 마스크 배열, 라벨을 반환합니다.\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "# 훈련 데이터와 라벨을 준비합니다.\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "# 테스트 데이터와 라벨을 준비합니다.\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "# 훈련 데이터 세트의 프레임 특징과 마스크의 차원을 출력합니다.\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "\"\"\"\n",
    "# 위 코드 블록의 실행 시간은 실행되는 컴퓨터에 따라 약 20분 정도 소요될 것입니다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f44cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 순차 모델 (Sequence Model)\n",
    "\n",
    "이제 `GRU`와 같은 순환 계층을 포함하는 순차 모델에 이 데이터를 공급할 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 순차 모델을 위한 유틸리티입니다.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    # 입력으로 프레임 특징과 마스크를 받습니다.\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # `mask` 사용의 중요성을 이해하기 위한 참고 자료:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    # 출력 계층은 분류할 클래스 수만큼의 노드를 가집니다.\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# 실험을 실행하기 위한 유틸리티입니다.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()\n",
    "\n",
    "\"\"\"\n",
    "**참고**: 이 예시를 상대적으로 빠른 시간 내에 실행하기 위해서, 몇 개의 훈련 예제만 사용했습니다.\n",
    "이는 순차 모델에 비해 적은 양의 훈련 예제입니다. UCF101 데이터셋에서 더 많은 데이터를 샘플링하고 \n",
    "동일한 모델을 훈련하기 위해서 위에서 언급된 노트북을 사용해 보시길 권장합니다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## 추론\n",
    "\"\"\"\n",
    "\n",
    "# 단일 비디오를 준비하기 위한 함수입니다.\n",
    "def prepare_single_video(frames):\n",
    "    # ... (중략) ...\n",
    "\n",
    "# 비디오의 시퀀스 예측을 수행하는 함수입니다.\n",
    "def sequence_prediction(path):\n",
    "    # ... (중략) ...\n",
    "\n",
    "# 이 유틸리티는 시각화를 위한 것입니다.\n",
    "# 참조된 자료:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    # ... (중략) ...\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "to_gif(test_frames[:MAX_SEQ_LENGTH])\n",
    "\n",
    "\"\"\"\n",
    "## 다음 단계\n",
    "\n",
    "* 이 예시에서 우리는 비디오 프레임에서 의미 있는 특징을 추출하기 위한 전이 학습(transfer learning)을 사용했습니다.\n",
    "  전이 학습된 네트워크를 미세 조정(fine-tuning)하여 그것이 결과에 어떻게 영향을 미치는지 확인할 수 있습니다.\n",
    "* 속도-정확도 균형을 맞추기 위해 `tf.keras.applications` 내의 다른 모델들을 시도해 볼 수 있습니다.\n",
    "* `MAX_SEQ_LENGTH`의 다양한 조합을 시도해 보고 성능에 어떻게 영향을 미치는지 관찰해보세요.\n",
    "* 더 많은 클래스 수로 훈련을 하고 좋은 성능을 얻을 수 있는지 시도해보세요.\n",
    "* [이 튜토리얼](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub)을 따라 DeepMind의 [사전 훈련된 동작 인식 모델](https://arxiv.org/abs/1705.07750)을 시도해보세요.\n",
    "* 롤링 평균(Rolling-averaging)은 비디오 분류에 유용한 기술일 수 있으며, 이는 표준 이미지 분류 모델과 함께 사용하여 비디오를 추론할 수 있습니다.\n",
    "  [이 튜토리얼](https://www.pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/)은 롤링 평균을 이미지 분류기와 함께 사용하는 방법을 이해하는 데 도움이 될 것입니다.\n",
    "* 비디오의 프레임 사이에 변동이 있을 때 모든 프레임이 카테고리를 결정하는 데 동등하게 중요하지 않을 수 있습니다.\n",
    "  그러한 상황에서 순차 모델에 [자체 주목 층(self-attention layer)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention)을 두면 더 나은 결과를 얻을 수 있습니다.\n",
    "* [이 책의 장](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11)을 따라 비디오를 처리하기 위한 Transformer 기반 모델을 구현할 수 있습니다.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
