{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d091c",
   "metadata": {},
   "outputs": [
    {
<<<<<<< Updated upstream:8. 실전프로젝트/231110test.ipynb
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_docs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embed\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paths\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_docs'"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
>>>>>>> Stashed changes:자습/8. 실전프로젝트/231110test.ipynb
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2793f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install imutils\n",
    "# !pip install opencv-python\n",
    "# !pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab70c7",
   "metadata": {},
   "source": [
    "### 학습데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c72518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/train/가볍다/1.mp4</td>\n",
       "      <td>가볍다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/train/가져오다/2.mp4</td>\n",
       "      <td>가져오다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/train/가짜/3.mp4</td>\n",
       "      <td>가짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/train/가치/4.mp4</td>\n",
       "      <td>가치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/train/보관/5.mp4</td>\n",
       "      <td>보관</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset/train/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset/train/보다/7.mp4</td>\n",
       "      <td>보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset/train/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset/train/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 video_name   tag\n",
       "0   dataset/train/가볍다/1.mp4   가볍다\n",
       "1  dataset/train/가져오다/2.mp4  가져오다\n",
       "2    dataset/train/가짜/3.mp4    가짜\n",
       "3    dataset/train/가치/4.mp4    가치\n",
       "4    dataset/train/보관/5.mp4    보관\n",
       "5   dataset/train/보내다/6.mp4   보내다\n",
       "6    dataset/train/보다/7.mp4    보다\n",
       "7    dataset/train/안경/8.mp4    안경\n",
       "8    dataset/train/알다/9.mp4    알다\n",
       "9  dataset/train/월요일/10.mp4   월요일"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/train')\n",
    "label_types = os.listdir('dataset/train')\n",
    "# 훈련 데이터셋을 위한 비어있는 리스트 초기화\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/train' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/train'+'/'+item)    \n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/train'+'/'+item)+'/'+room))\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name']).loc[:,['video_name','tag']]\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('train.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8eb6bd",
   "metadata": {},
   "source": [
    "### 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca461d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/test/가볍다/1.mp4</td>\n",
       "      <td>가볍다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/test/가져오다/2.mp4</td>\n",
       "      <td>가져오다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/test/가짜/3.mp4</td>\n",
       "      <td>가짜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/test/가치/4.mp4</td>\n",
       "      <td>가치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/test/보관/5.mp4</td>\n",
       "      <td>보관</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset/test/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset/test/보다/7.mp4</td>\n",
       "      <td>보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset/test/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset/test/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset/test/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                video_name   tag\n",
       "0   dataset/test/가볍다/1.mp4   가볍다\n",
       "1  dataset/test/가져오다/2.mp4  가져오다\n",
       "2    dataset/test/가짜/3.mp4    가짜\n",
       "3    dataset/test/가치/4.mp4    가치\n",
       "4    dataset/test/보관/5.mp4    보관\n",
       "5   dataset/test/보내다/6.mp4   보내다\n",
       "6    dataset/test/보다/7.mp4    보다\n",
       "7    dataset/test/안경/8.mp4    안경\n",
       "8    dataset/test/알다/9.mp4    알다\n",
       "9  dataset/test/월요일/10.mp4   월요일"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "room_types = os.listdir('dataset/test')\n",
    "\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/test' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/test'+'/'+item)\n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/test'+'/'+item)+'/'+room))\n",
    "\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name'])\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('test.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cbe40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9c5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# GPU가 사용 가능한 경우\n",
    "if gpus:\n",
    "    try:\n",
    "        # 첫 번째 GPU에 대해 메모리 제한 설정\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0], \n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40e6d6",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9ea385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video for training: 10\n",
      "Total video for testing: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dataset/train/보관/5.mp4</td>\n",
       "      <td>보관</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dataset/train/월요일/10.mp4</td>\n",
       "      <td>월요일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dataset/train/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dataset/train/가치/4.mp4</td>\n",
       "      <td>가치</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                video_name  tag\n",
       "4           4    dataset/train/보관/5.mp4   보관\n",
       "9           9  dataset/train/월요일/10.mp4  월요일\n",
       "5           5   dataset/train/보내다/6.mp4  보내다\n",
       "7           7    dataset/train/안경/8.mp4   안경\n",
       "3           3    dataset/train/가치/4.mp4   가치"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋과 테스트 데이터셋을 각각 CSV 파일에서 로드\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 훈련 및 테스트 데이터셋 크기 출력\n",
    "print(f\"Total video for training: {len(train_df)}\")\n",
    "print(f\"Total video for testing: {len(test_df)}\")\n",
    "\n",
    "# 훈련 데이터셋의 샘플 5개 출력\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1964752",
   "metadata": {},
   "source": [
    "### Feed the video to a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b3304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c95908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지에서 중앙에 맞춰 정사각형으로 잘나내는 함수\n",
    "def crop_center_square(frame):\n",
    "    # 이미지의 높이(y)와 너비(x)를 가져옴\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 이미지의 높이와 너비 중 더 작은 값을 선택하여 정사각형의 크기를 결정\n",
    "    min_dim = min(y, x)\n",
    "    # 정사각형을 이미지 중앙에 위치시키기 위해 시작점의 x좌표와 y좌표를 계산\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 계산된 시작점과 정사각형의 크기를 이용하여 이미지의 중앙 부분을 잘라냅니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "# 비디오 파일을 로드하고, 각 프레임을 처리하여 배열로 반환하는 함수\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    # OpenCV를 사용하여 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    skeletons = []  # 스켈레톤 데이터\n",
    "    hand_landmarks = []  # 손 데이터\n",
    "    face_landmarks = []  # 얼굴 데이터\n",
    "    try:\n",
    "        while True:\n",
    "            # 비디오에서 프레임을 하나씩 읽기\n",
    "            ret, frame = cap.read()\n",
    "            # 읽을 프레임이 없으면 반복문을 종료\n",
    "            if not ret:\n",
    "                break\n",
    "            # 읽은 프레임에서 중앙의 정사각형 부분을 잘라냄\n",
    "            frame = crop_center_square(frame)\n",
    "            # 프레임의 크기를 지정된 크기로 조절\n",
    "            frame = cv2.resize(frame, resize)            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Mediapipe를 사용하여 스켈레톤 추출\n",
    "            hands_results = hands.process(frame_rgb)\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            face_results = face_detection.process(frame_rgb)\n",
    "           \n",
    "            if pose_results.pose_landmarks:\n",
    "                skeletons.append(pose_results.pose_landmarks.landmark)\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                hand_landmarks.append(hands_results.multi_hand_landmarks)\n",
    "            if face_results.detections:\n",
    "                face_landmarks.append(face_results.detections)\n",
    "            \n",
    "            # OpenCV는 BGR 색상 순서를 사용하므로, 이를 RGB 순서로 변경\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            # 처리된 프레임을 프레임 리스트에 추가\n",
    "            frames.append(frame)\n",
    "            # max_frames가 지정된 경우, 지정된 수의 프레임만큼만 처리\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        # 비디오 파일을 닫기\n",
    "        cap.release()\n",
    "        pose.close\n",
    "        hands.close\n",
    "        face_detection.close\n",
    "    # 처리된 모든 프레임을 numpy 배열로 변환하여 반환\n",
    "    return np.array(frames), skeletons, hand_landmarks, face_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99e92e",
   "metadata": {},
   "source": [
    "### 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f389d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    # 이미지 특징 추출을 위한 InceptionV3 모델\n",
    "    base_model = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    image_input = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed_image = preprocess_input(image_input)\n",
    "    image_features = base_model(preprocessed_image)\n",
    "\n",
    "    # Mediapipe 데이터를 위한 입력 레이어 및 처리 레이어\n",
    "    # 예시로, Mediapipe 데이터의 차원을 상정하여 입력 레이어를 정의\n",
    "    mediapipe_input = keras.Input((1599+63,))\n",
    "    mediapipe_features = keras.layers.Dense(128, activation=\"relu\")(mediapipe_input)\n",
    "\n",
    "    # 이미지 특징과 Mediapipe 데이터의 결합\n",
    "    combined_features = keras.layers.concatenate([image_features, mediapipe_features])\n",
    "\n",
    "    # 최종 모델\n",
    "    outputs = keras.layers.Dense(10, activation=\"softmax\")(combined_features)\n",
    "    return keras.Model(inputs=[image_input, mediapipe_input], outputs=outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0fc33",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "- StringLookup layer encode the class labels as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc6ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "['가볍다', '가져오다', '가짜', '가치', '보관', '보내다', '보다', '안경', '알다', '월요일']\n",
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[...,None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "SKELETON_FEATURES = 33*4\n",
    "HAND_FEATURES = 21*3*2\n",
    "FACE_FEATURES = 468*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a589e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame feature in train set: (10, 20, 2048)\n",
      "Frame masks in train set: (10, 20, 132)\n",
      "train_labels in train set:(10,)\n",
      "test_labels in train set:(10,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_video(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # 수정: Mediapipe 데이터를 저장할 배열 초기화\n",
    "    frame_skeletons = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "    frame_faces = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, FACE_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames, skeletons, hands, faces = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        video_length = min(MAX_SEQ_LENGTH, frames.shape[1])\n",
    "\n",
    "        for i in range(video_length):\n",
    "            frame_features[idx, i, :] = feature_extractor.predict(frames[0, i, :])\n",
    "\n",
    "            # Mediapipe 데이터의 전처리 및 저장\n",
    "            frame_skeletons[idx, i, :] = preprocess_skeleton_data(skeletons[i])\n",
    "            frame_hands[idx, i, :] = preprocess_hand_data(hands[i])\n",
    "            frame_faces[idx, i, :] = preprocess_face_data(faces[i])\n",
    "\n",
    "            frame_masks[idx, i] = 1\n",
    "\n",
    "    # 수정: 반환 값에 Mediapipe 데이터 포함\n",
    "    return (frame_features, frame_skeletons, frame_hands, frame_faces, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_video(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_video(test_df,\"test\")\n",
    "train_labels = np.squeeze(train_labels)\n",
    "test_labels = np.squeeze(test_labels)\n",
    "\n",
    "print(f\"Frame feature in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "print(f\"train_labels in train set:{train_labels.shape}\")\n",
    "print(f\"test_labels in train set:{test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3683c5",
   "metadata": {},
   "source": [
    "### The sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f9cfda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 2.3035 - accuracy: 0.0000e+00 \n",
      "Epoch 1: val_loss improved from inf to 2.30705, saving model to ./tmp\\video_classifier.h5\n",
      "8/8 [==============================] - 9s 319ms/step - loss: 2.3040 - accuracy: 0.0000e+00 - val_loss: 2.3071 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.3014 - accuracy: 0.1429\n",
      "Epoch 2: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.3012 - accuracy: 0.1250 - val_loss: 2.3138 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2998 - accuracy: 0.1429    \n",
      "Epoch 3: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2994 - accuracy: 0.1250 - val_loss: 2.3207 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2984 - accuracy: 0.0000e+00\n",
      "Epoch 4: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2977 - accuracy: 0.1250 - val_loss: 2.3275 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2964 - accuracy: 0.1429    \n",
      "Epoch 5: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2961 - accuracy: 0.1250 - val_loss: 2.3343 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2945 - accuracy: 0.1250    \n",
      "Epoch 6: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2945 - accuracy: 0.1250 - val_loss: 2.3412 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2926 - accuracy: 0.1429    \n",
      "Epoch 7: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2928 - accuracy: 0.1250 - val_loss: 2.3480 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2912 - accuracy: 0.1250    \n",
      "Epoch 8: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2912 - accuracy: 0.1250 - val_loss: 2.3548 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2894 - accuracy: 0.1429    \n",
      "Epoch 9: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2896 - accuracy: 0.1250 - val_loss: 2.3616 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2886 - accuracy: 0.0000e+00\n",
      "Epoch 10: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2880 - accuracy: 0.1250 - val_loss: 2.3684 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2869 - accuracy: 0.1429    \n",
      "Epoch 11: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2865 - accuracy: 0.1250 - val_loss: 2.3752 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2851 - accuracy: 0.1429    \n",
      "Epoch 12: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2849 - accuracy: 0.1250 - val_loss: 2.3819 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2833 - accuracy: 0.1250    \n",
      "Epoch 13: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2833 - accuracy: 0.1250 - val_loss: 2.3887 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2818 - accuracy: 0.1250    \n",
      "Epoch 14: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2818 - accuracy: 0.1250 - val_loss: 2.3954 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2803 - accuracy: 0.1429    \n",
      "Epoch 15: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2804 - accuracy: 0.1250 - val_loss: 2.4022 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2795 - accuracy: 0.0000e+00\n",
      "Epoch 16: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2788 - accuracy: 0.1250 - val_loss: 2.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2778 - accuracy: 0.1429    \n",
      "Epoch 17: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2774 - accuracy: 0.1250 - val_loss: 2.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2759 - accuracy: 0.1250    \n",
      "Epoch 18: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2759 - accuracy: 0.1250 - val_loss: 2.4223 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2745 - accuracy: 0.1250    \n",
      "Epoch 19: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2745 - accuracy: 0.1250 - val_loss: 2.4289 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2731 - accuracy: 0.1250    \n",
      "Epoch 20: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2731 - accuracy: 0.1250 - val_loss: 2.4356 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2716 - accuracy: 0.1250    \n",
      "Epoch 21: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2716 - accuracy: 0.1250 - val_loss: 2.4422 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2702 - accuracy: 0.1250    \n",
      "Epoch 22: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2702 - accuracy: 0.1250 - val_loss: 2.4489 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2689 - accuracy: 0.1250    \n",
      "Epoch 23: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2689 - accuracy: 0.1250 - val_loss: 2.4555 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2675 - accuracy: 0.1250    \n",
      "Epoch 24: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2675 - accuracy: 0.1250 - val_loss: 2.4621 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2661 - accuracy: 0.1250    \n",
      "Epoch 25: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2661 - accuracy: 0.1250 - val_loss: 2.4687 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2645 - accuracy: 0.1429    \n",
      "Epoch 26: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2648 - accuracy: 0.1250 - val_loss: 2.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2635 - accuracy: 0.1429    \n",
      "Epoch 27: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2635 - accuracy: 0.1250 - val_loss: 2.4818 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2621 - accuracy: 0.1250    \n",
      "Epoch 28: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2621 - accuracy: 0.1250 - val_loss: 2.4884 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.2609 - accuracy: 0.1429    \n",
      "Epoch 29: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.2609 - accuracy: 0.1250 - val_loss: 2.4949 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.2595 - accuracy: 0.1250    \n",
      "Epoch 30: val_loss did not improve from 2.30705\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2.2595 - accuracy: 0.1250 - val_loss: 2.5014 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Test accuracy: 10.0%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    # 기존 이미지 특징에 대한 입력\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    # Mediapipe 데이터에 대한 추가 입력 레이어\n",
    "    skeleton_input = keras.Input((MAX_SEQ_LENGTH, SKELETON_FEATURES))\n",
    "    hand_input = keras.Input((MAX_SEQ_LENGTH, HAND_FEATURES))\n",
    "    face_input = keras.Input((MAX_SEQ_LENGTH, FACE_FEATURES))\n",
    "    \n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    \n",
    "    # 이미지 특징 처리를 위한 GRU 레이어\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8, return_sequences=True)(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)  # 형태 변경\n",
    "    \n",
    "    # Mediapipe 데이터를 처리하는 추가 네트워크 레이어 (예시)\n",
    "    # 여기서는 간단히 Dense 레이어를 사용했지만, 필요에 따라 다른 구조를 사용할 수 있습니다.\n",
    "    y_skeleton = keras.layers.GlobalAveragePooling1D()(skeleton_input)\n",
    "    y_hand = keras.layers.GlobalAveragePooling1D()(hand_input)\n",
    "    y_face = keras.layers.GlobalAveragePooling1D()(face_input)\n",
    "\n",
    "    # 모든 특징을 결합\n",
    "    combined = keras.layers.concatenate([x, y_skeleton, y_hand, y_face])\n",
    "\n",
    "    # 결합된 특징에 대한 추가 처리\n",
    "    z = keras.layers.Dense(16, activation=\"relu\")(combined)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(z)\n",
    "    \n",
    "    rnn_model = keras.Model([frame_features_input, skeleton_input, hand_input, face_input, mask_input], output)\n",
    "    rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1], train_data[2], train_data[3], train_data[4]],  # 수정된 입력 데이터\n",
    "        train_labels,\n",
    "        batch_size=1,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate(\n",
    "        [test_data[0], test_data[1], test_data[2], test_data[3], test_data[4]],  # 수정된 입력 데이터\n",
    "        test_labels\n",
    "    )\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6f9bb",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd65768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path : dataset/test/보다/7.mp4\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "가져오다 : 10.06%\n",
      "가치 : 10.04%\n",
      "보내다 : 10.03%\n",
      "안경 : 10.02%\n",
      "보다 : 10.00%\n",
      "가볍다 :  9.99%\n",
      "보관 :  9.98%\n",
      "가짜 :  9.97%\n",
      "월요일 :  9.96%\n",
      "알다 :  9.96%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames, skeletons, hands, faces):\n",
    "    frames = frames[None,...]\n",
    "    frame_mask = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Mediapipe 데이터를 위한 배열 초기화\n",
    "    frame_skeletons = np.zeros((1, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros((1, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "    frame_faces = np.zeros((1, MAX_SEQ_LENGTH, FACE_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    video_length = min(MAX_SEQ_LENGTH, frames.shape[1])\n",
    "\n",
    "    for j in range(video_length):\n",
    "        frame_features[0, j, :] = feature_extractor.predict(frames[0, j, :])\n",
    "\n",
    "        # Mediapipe 데이터 처리 및 저장\n",
    "        frame_skeletons[0, j, :] = preprocess_skeleton_data(skeletons[j])\n",
    "        frame_hands[0, j, :] = preprocess_hand_data(hands[j])\n",
    "        frame_faces[0, j, :] = preprocess_face_data(faces[j])\n",
    "\n",
    "        frame_mask[0, j] = 1\n",
    "\n",
    "    return frame_features, frame_skeletons, frame_hands, frame_faces, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    frames, skeletons, hands, faces = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_skeletons, frame_hands, frame_faces, frame_mask = prepare_single_video(frames, skeletons, hands, faces)\n",
    "\n",
    "    probabilities = sequence_model.predict([frame_features, frame_skeletons, frame_hands, frame_faces, frame_mask])[0]\n",
    "    \n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"{class_vocab[i]} : {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "    \n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path : {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555b358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
