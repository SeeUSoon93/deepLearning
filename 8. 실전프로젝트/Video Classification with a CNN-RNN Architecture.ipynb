{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가볍다', '가져오다', '가짜', '가치', '보관', '보내다', '보다', '안경', '알다', '월요일']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset/train')\n",
    "\n",
    "label_types = os.listdir('dataset/train')\n",
    "print(label_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab70c7",
   "metadata": {},
   "source": [
    "### 학습데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c72518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>가볍다</td>\n",
       "      <td>dataset/train/가볍다/1.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>가져오다</td>\n",
       "      <td>dataset/train/가져오다/2.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>가짜</td>\n",
       "      <td>dataset/train/가짜/3.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>가치</td>\n",
       "      <td>dataset/train/가치/4.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>보관</td>\n",
       "      <td>dataset/train/보관/5.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>보내다</td>\n",
       "      <td>dataset/train/보내다/6.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>보다</td>\n",
       "      <td>dataset/train/보다/7.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>안경</td>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>알다</td>\n",
       "      <td>dataset/train/알다/9.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>월요일</td>\n",
       "      <td>dataset/train/월요일/10.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tag                video_name\n",
       "0   가볍다   dataset/train/가볍다/1.mp4\n",
       "1  가져오다  dataset/train/가져오다/2.mp4\n",
       "2    가짜    dataset/train/가짜/3.mp4\n",
       "3    가치    dataset/train/가치/4.mp4\n",
       "4    보관    dataset/train/보관/5.mp4\n",
       "5   보내다   dataset/train/보내다/6.mp4\n",
       "6    보다    dataset/train/보다/7.mp4\n",
       "7    안경    dataset/train/안경/8.mp4\n",
       "8    알다    dataset/train/알다/9.mp4\n",
       "9   월요일  dataset/train/월요일/10.mp4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rooms = []\n",
    "for item in dataset_path:\n",
    "    # 모든 파일 이름 가져오기\n",
    "    all_rooms = os.listdir('dataset/train'+'/'+item)\n",
    "    \n",
    "    # 리스트에 더하기\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/train'+'/'+item)+'/'+room))\n",
    "\n",
    "# Build a dataframe\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a49189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df.to_csv('train.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8eb6bd",
   "metadata": {},
   "source": [
    "### 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "\n",
    "room_types = os.listdir('dataset/test')\n",
    "\n",
    "rooms = []\n",
    "for item in dataset_path:\n",
    "    # 모든 파일 이름 가져오기\n",
    "    all_rooms = os.listdir('dataset/test'+'/'+item)\n",
    "    \n",
    "    # 리스트에 더하기\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/test'+'/'+item)+'/'+room))\n",
    "\n",
    "# Build a dataframe\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name'])\n",
    "\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df.to_csv('test.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cbe40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830d834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9c5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0], \n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40e6d6",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9ea385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video for training: 10\n",
      "Total video for testing: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dataset/train/가져오다/2.mp4</td>\n",
       "      <td>가져오다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dataset/train/보내다/6.mp4</td>\n",
       "      <td>보내다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dataset/train/안경/8.mp4</td>\n",
       "      <td>안경</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dataset/train/보다/7.mp4</td>\n",
       "      <td>보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dataset/train/알다/9.mp4</td>\n",
       "      <td>알다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                video_name   tag\n",
       "1           1  dataset/train/가져오다/2.mp4  가져오다\n",
       "5           5   dataset/train/보내다/6.mp4   보내다\n",
       "7           7    dataset/train/안경/8.mp4    안경\n",
       "6           6    dataset/train/보다/7.mp4    보다\n",
       "8           8    dataset/train/알다/9.mp4    알다"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total video for training: {len(train_df)}\")\n",
    "print(f\"Total video for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1964752",
   "metadata": {},
   "source": [
    "### Feed the video to a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c95908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y,x = frame.shape[0:2]\n",
    "    min_dim = min(y,x)\n",
    "    start_x = (x//2)-(min_dim//2)\n",
    "    start_y = (y//2)-(min_dim//2)\n",
    "    return frame[start_y : start_y +min_dim, start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)  \n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:,:,[2,1,0]]\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99e92e",
   "metadata": {},
   "source": [
    "### 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f389d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),)\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    \n",
    "    inputs = keras.Input((IMG_SIZE,IMG_SIZE,3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    \n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0fc33",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "- StringLookup layer encode the class labels as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc6ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가볍다', '가져오다', '가짜', '가치', '보관', '보내다', '보다', '안경', '알다', '월요일']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[...,None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951903ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a589e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame feature in train set: (10, 20, 2048)\n",
      "Frame masks in train set: (10, 20)\n",
      "train_labels in train set:(10, 1)\n",
      "test_labels in train set:(10, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_video(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    \n",
    "    labels = df[\"tag\"].values\n",
    "    \n",
    "    labels = label_processor(labels[...,None]).numpy()\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples,MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_fetures = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES),dtype=\"float32\")\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir,path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1,MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_fetures = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES),dtype=\"float32\")\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_fetures[i,j,:] = feature_extractor.predict(\n",
    "                    batch[None,j,:]\n",
    "                )\n",
    "            temp_frame_mask[i,:length] = 1\n",
    "            \n",
    "        frame_fetures[idx,] = temp_frame_fetures.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "    \n",
    "    return (frame_fetures, frame_masks), labels\n",
    "\n",
    "train_data, train_labels = prepare_all_video(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_video(test_df,\"test\")\n",
    "\n",
    "print(f\"Frame feature in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "print(f\"train_labels in train set:{train_labels.shape}\")\n",
    "print(f\"test_labels in train set:{test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3683c5",
   "metadata": {},
   "source": [
    "### The sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f9cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.1429\n",
      "Epoch 1: val_loss improved from inf to 2.30399, saving model to ./tmp\\video_classifier.h5\n",
      "1/1 [==============================] - 13s 13s/step - loss: 2.3026 - accuracy: 0.1429 - val_loss: 2.3040 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3020 - accuracy: 0.1429\n",
      "Epoch 2: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3020 - accuracy: 0.1429 - val_loss: 2.3054 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3014 - accuracy: 0.1429\n",
      "Epoch 3: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.3014 - accuracy: 0.1429 - val_loss: 2.3068 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3008 - accuracy: 0.1429\n",
      "Epoch 4: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.3008 - accuracy: 0.1429 - val_loss: 2.3082 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3002 - accuracy: 0.1429\n",
      "Epoch 5: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.3002 - accuracy: 0.1429 - val_loss: 2.3096 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2996 - accuracy: 0.1429\n",
      "Epoch 6: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2996 - accuracy: 0.1429 - val_loss: 2.3110 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2990 - accuracy: 0.1429\n",
      "Epoch 7: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2990 - accuracy: 0.1429 - val_loss: 2.3124 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2984 - accuracy: 0.1429\n",
      "Epoch 8: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2984 - accuracy: 0.1429 - val_loss: 2.3138 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2978 - accuracy: 0.1429\n",
      "Epoch 9: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2978 - accuracy: 0.1429 - val_loss: 2.3152 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2972 - accuracy: 0.1429\n",
      "Epoch 10: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.2972 - accuracy: 0.1429 - val_loss: 2.3166 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2966 - accuracy: 0.1429\n",
      "Epoch 11: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2966 - accuracy: 0.1429 - val_loss: 2.3180 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2960 - accuracy: 0.1429\n",
      "Epoch 12: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2960 - accuracy: 0.1429 - val_loss: 2.3194 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2955 - accuracy: 0.1429\n",
      "Epoch 13: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2955 - accuracy: 0.1429 - val_loss: 2.3208 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2949 - accuracy: 0.1429\n",
      "Epoch 14: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2949 - accuracy: 0.1429 - val_loss: 2.3222 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2943 - accuracy: 0.1429\n",
      "Epoch 15: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2943 - accuracy: 0.1429 - val_loss: 2.3237 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2937 - accuracy: 0.1429\n",
      "Epoch 16: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2937 - accuracy: 0.1429 - val_loss: 2.3251 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2931 - accuracy: 0.1429\n",
      "Epoch 17: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2931 - accuracy: 0.1429 - val_loss: 2.3265 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2925 - accuracy: 0.1429\n",
      "Epoch 18: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2925 - accuracy: 0.1429 - val_loss: 2.3279 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2919 - accuracy: 0.1429\n",
      "Epoch 19: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2919 - accuracy: 0.1429 - val_loss: 2.3293 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2914 - accuracy: 0.1429\n",
      "Epoch 20: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2914 - accuracy: 0.1429 - val_loss: 2.3307 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2908 - accuracy: 0.1429\n",
      "Epoch 21: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2908 - accuracy: 0.1429 - val_loss: 2.3321 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2902 - accuracy: 0.1429\n",
      "Epoch 22: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2902 - accuracy: 0.1429 - val_loss: 2.3335 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2896 - accuracy: 0.1429\n",
      "Epoch 23: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.2896 - accuracy: 0.1429 - val_loss: 2.3349 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2890 - accuracy: 0.1429\n",
      "Epoch 24: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2890 - accuracy: 0.1429 - val_loss: 2.3363 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2885 - accuracy: 0.1429\n",
      "Epoch 25: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.2885 - accuracy: 0.1429 - val_loss: 2.3378 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2879 - accuracy: 0.1429\n",
      "Epoch 26: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.2879 - accuracy: 0.1429 - val_loss: 2.3392 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2873 - accuracy: 0.1429\n",
      "Epoch 27: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.2873 - accuracy: 0.1429 - val_loss: 2.3406 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2867 - accuracy: 0.1429\n",
      "Epoch 28: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2867 - accuracy: 0.1429 - val_loss: 2.3420 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2862 - accuracy: 0.1429\n",
      "Epoch 29: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2862 - accuracy: 0.1429 - val_loss: 2.3434 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2856 - accuracy: 0.1429\n",
      "Epoch 30: val_loss did not improve from 2.30399\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2856 - accuracy: 0.1429 - val_loss: 2.3448 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Test accuracy:10.0%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    \n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab),activation=\"softmax\")(x)\n",
    "    \n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "    \n",
    "    rnn_model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    \n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "def run_experiment():\n",
    "\n",
    "    filepath = \"./tmp/video_classifier.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "    \n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "    \n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy:{round(accuracy*100,2)}%\")\n",
    "    \n",
    "    return history, seq_model\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6f9bb",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd65768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path : dataset/test/월요일/10.mp4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "보다 : 10.01%\n",
      "보내다 : 10.01%\n",
      "보관 : 10.01%\n",
      "가치 : 10.01%\n",
      "가짜 : 10.01%\n",
      "가져오다 : 10.01%\n",
      "가볍다 : 10.01%\n",
      "안경 :  9.99%\n",
      "월요일 :  9.99%\n",
      "알다 :  9.99%\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None,...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    \n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i,j,:] = feature_extractor.predict(batch[None,j,:])\n",
    "        frame_mask[i,:length]=1\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "    \n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"{class_vocab[i]} : {probabilities[i]*100:5.2f}%\")\n",
    "    return frames\n",
    "    \n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path : {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde65ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708942fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad0191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c168d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba8908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136825d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9464b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93009536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d984b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddce36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dad47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72220442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e8eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058ad7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fd633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65843333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555b358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
