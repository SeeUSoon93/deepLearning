{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2750b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.applications import ResNet50\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6857ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'E:/군순/train'\n",
    "test_dir = 'E:/군순/train'\n",
    "# 비디오 파일 목록과 태그를 포함하는 리스트를 만드는 함수\n",
    "def create_data_list(data_dir):\n",
    "    data_list = []\n",
    "    # data_dir 안의 각 디렉토리에 대해 반복\n",
    "    for item in os.listdir(data_dir):\n",
    "        item_path = os.path.join(data_dir, item)  # 아이템의 전체 경로\n",
    "        # 해당 경로가 디렉토리인지 확인\n",
    "        if os.path.isdir(item_path):\n",
    "            # 디렉토리 내의 모든 파일을 나열\n",
    "            for file_name in os.listdir(item_path):\n",
    "                # 파일이 .mp4 파일인지 확인\n",
    "                if file_name.endswith('.mp4'):\n",
    "                    # 리스트에 태그와 파일 경로를 추가\n",
    "                    data_list.append((item, str('E:/군순/train'+'/'+item)+'/'+file_name))\n",
    "    return data_list\n",
    "\n",
    "# 함수를 사용해서 리스트를 생성\n",
    "train_list = create_data_list(train_dir)\n",
    "test_list = create_data_list(test_dir)\n",
    "# 리스트에서 데이터프레임을 생성\n",
    "train_df = pd.DataFrame(data=train_list, columns=['tag', 'video_name'])\n",
    "test_df = pd.DataFrame(data=test_list, columns=['tag', 'video_name'])\n",
    "# 필요한 경우 열 순서를 수정\n",
    "train_df = train_df.loc[:, ['tag', 'video_name']]\n",
    "test_df = test_df.loc[:, ['tag', 'video_name']]\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "train_df.to_csv(train_file_path, encoding='utf-8-sig', index=False)\n",
    "test_df.to_csv(test_file_path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1756281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video for training: 75\n",
      "Total video for testing: 75\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"Total video for training: {len(train_df)}\")\n",
    "print(f\"Total video for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a9b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "SKELETON_FEATURES = 33*4\n",
    "HAND_FEATURES = 21*3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0052228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rlarn\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라벨링\n",
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b79c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지에서 중앙에 맞춰 정사각형으로 잘나내는 함수\n",
    "def crop_center_square(frame):\n",
    "    # 이미지의 높이(y)와 너비(x)를 가져옴\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 이미지의 높이와 너비 중 더 작은 값을 선택하여 정사각형의 크기를 결정\n",
    "    min_dim = min(y, x)\n",
    "    # 정사각형을 이미지 중앙에 위치시키기 위해 시작점의 x좌표와 y좌표를 계산\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 계산된 시작점과 정사각형의 크기를 이용하여 이미지의 중앙 부분을 잘라냅니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f81fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 파일을 로드하고, 각 프레임을 처리하여 배열로 반환하는 함수\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    # OpenCV를 사용하여 비디오 파일 열기\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    skeletons = []  # 스켈레톤 데이터\n",
    "    hand_landmarks = []  # 손 데이터\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # 비디오에서 프레임을 하나씩 읽기\n",
    "            ret, frame = cap.read()\n",
    "            # 읽을 프레임이 없으면 반복문을 종료\n",
    "            if not ret:\n",
    "                break\n",
    "            # 읽은 프레임에서 중앙의 정사각형 부분을 잘라냄\n",
    "            frame = crop_center_square(frame)\n",
    "            # 프레임의 크기를 지정된 크기로 조절\n",
    "            frame = cv2.resize(frame, resize)            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Mediapipe를 사용하여 스켈레톤 추출\n",
    "            hands_results = hands.process(frame_rgb)\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "           \n",
    "            if pose_results.pose_landmarks:\n",
    "                skeletons.append(pose_results.pose_landmarks.landmark)\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                hand_landmarks.append(hands_results.multi_hand_landmarks)\n",
    "            \n",
    "            # OpenCV는 BGR 색상 순서를 사용하므로, 이를 RGB 순서로 변경\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            # 처리된 프레임을 프레임 리스트에 추가\n",
    "            frames.append(frame)\n",
    "            # max_frames가 지정된 경우, 지정된 수의 프레임만큼만 처리\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        # 비디오 파일을 닫기\n",
    "        cap.release()\n",
    "        pose.close\n",
    "        hands.close\n",
    "    return np.array(frames), skeletons, hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징추출\n",
    "def build_feature_extractor():\n",
    "    # 이미지 특징 추출을 위한 InceptionV3 모델\n",
    "    base_model = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    image_input = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed_image = preprocess_input(image_input)\n",
    "    image_features = base_model(preprocessed_image)\n",
    "\n",
    "    # Mediapipe 데이터를 위한 입력 레이어 및 처리 레이어\n",
    "    # 예시로, Mediapipe 데이터의 차원을 상정하여 입력 레이어를 정의\n",
    "    mediapipe_input = keras.Input((258,))\n",
    "    mediapipe_features = keras.layers.Dense(258, activation=\"relu\")(mediapipe_input)\n",
    "\n",
    "    # 이미지 특징과 Mediapipe 데이터의 결합\n",
    "    combined_features = keras.layers.concatenate([image_features, mediapipe_features])\n",
    "\n",
    "    # 최종 모델\n",
    "    outputs = keras.layers.Dense(15, activation=\"softmax\")(combined_features)\n",
    "    return keras.Model(inputs=[image_input, mediapipe_input], outputs=outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a307a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손 랜드마크를 2개로 제한한 코드\n",
    "def preprocess_skeleton_data(skeleton):\n",
    "    # 스켈레톤 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not skeleton:\n",
    "        return np.zeros(SKELETON_FEATURES)\n",
    "    # 스켈레톤 데이터를 1차원 배열로 변환\n",
    "    skeleton_array = np.array([[lm.x, lm.y, lm.z] for lm in skeleton]).flatten()    \n",
    "    # 부족한 부분을 0으로 채우기\n",
    "    skeleton_array = np.pad(skeleton_array, ((0, max(0, SKELETON_FEATURES - len(skeleton_array)))) )    \n",
    "    return skeleton_array\n",
    "\n",
    "def preprocess_hand_data(hand_landmarks):\n",
    "    # 손 랜드마크 데이터가 없는 경우 빈 벡터 반환\n",
    "    if not hand_landmarks or len(hand_landmarks) < 2:\n",
    "        return np.zeros(HAND_FEATURES)\n",
    "    \n",
    "    # 첫 번째와 두 번째 손에 대한 랜드마크만 처리\n",
    "    hand_data = []\n",
    "    for i in range(2):\n",
    "        hand_lm = hand_landmarks[i]\n",
    "        lm_array = np.array([[lm.x, lm.y, lm.z] for lm in hand_lm.landmark]).flatten()\n",
    "        hand_data.extend(lm_array)\n",
    "\n",
    "    # 부족한 부분을 0으로 채우기\n",
    "    hand_data = np.pad(hand_data, ((0, max(0, HAND_FEATURES - len(hand_data)))))\n",
    "\n",
    "    return np.array(hand_data)\n",
    "\n",
    "def preprocess_image(frame):    \n",
    "    frame = image.img_to_array(frame[0])  # frame[0]으로 변경\n",
    "    frame = preprocess_input(frame)  # ResNet50의 전처리 함수를 사용하여 정규화\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae3a9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_all_video(df):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "\n",
    "    # Mediapipe 데이터를 저장할 배열 초기화\n",
    "    frame_skeletons = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    \n",
    "    # 이미지 데이터 저장할 배열 초기화\n",
    "    frame_images = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=\"float32\")\n",
    "    \n",
    "    # 특징 추출기 모델 초기화\n",
    "    feature_extractor = build_feature_extractor()\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames, skeletons, hands = load_video(path)\n",
    "        video_length = min(MAX_SEQ_LENGTH, frames.shape[1])\n",
    "\n",
    "        for i in range(video_length):\n",
    "            # 이미지 데이터 전처리 및 특징 추출\n",
    "            image_feature = preprocess_image(frames[i:i+1])\n",
    "            image_feature = np.expand_dims(image_feature, axis=0)  # 차원 확장\n",
    "            \n",
    "            # InceptionV3 모델에 전처리된 이미지 전달\n",
    "            image_feature = feature_extractor.layers[0](image_feature)            \n",
    "            \n",
    "            # Mediapipe 데이터 전처리\n",
    "            skeleton_feature = preprocess_skeleton_data(skeletons[i])\n",
    "            hand_feature = preprocess_hand_data(hands[i])\n",
    "            combined_mediapipe_data = np.concatenate([skeleton_feature, hand_feature])\n",
    "            \n",
    "            # 차원 확장\n",
    "            combined_mediapipe_data = np.expand_dims(combined_mediapipe_data, axis=0)\n",
    "            \n",
    "            # 크기 확인\n",
    "            print(\"Image Feature shape:\", image_feature.shape)\n",
    "            print(\"Combined Mediapipe Data shape:\", combined_mediapipe_data.shape)\n",
    "            \n",
    "            # 데이터 저장\n",
    "            frame_images[idx, i, :] = image_feature           \n",
    "\n",
    "            # Mediapipe 데이터의 전처리 및 저장\n",
    "            frame_skeletons[idx, i, :] = skeleton_feature\n",
    "            frame_hands[idx, i, :] = hand_feature\n",
    "\n",
    "            frame_masks[idx, i] = 1\n",
    "            \n",
    "            # 모델 예측\n",
    "            try:\n",
    "                frame_feature = feature_extractor.predict([image_feature, combined_mediapipe_data])\n",
    "                print(\"Prediction shape:\", frame_feature.shape)\n",
    "            except Exception as e:\n",
    "                print(\"Error during prediction:\", e)\n",
    "\n",
    "    # 반환 값에 Mediapipe 데이터 포함\n",
    "    return (frame_features, frame_skeletons, frame_hands, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = prepare_all_video(train_df)\n",
    "test_data, test_labels = train_data, train_labels\n",
    "train_labels = np.squeeze(train_labels)\n",
    "test_labels = np.squeeze(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    \n",
    "    # 기존 이미지 특징에 대한 입력\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    # Mediapipe 데이터에 대한 추가 입력 레이어\n",
    "    skeleton_input = keras.Input((MAX_SEQ_LENGTH, SKELETON_FEATURES))\n",
    "    hand_input = keras.Input((MAX_SEQ_LENGTH, HAND_FEATURES))    \n",
    "    \n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    \n",
    "    # 이미지 특징 처리를 위한 GRU 레이어\n",
    "    x = keras.layers.GRU(64, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(32, return_sequences=True)(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)  # 형태 변경\n",
    "    \n",
    "    # Mediapipe 데이터를 처리하는 추가 네트워크 레이어 (예시)\n",
    "    # 여기서는 간단히 Dense 레이어를 사용했지만, 필요에 따라 다른 구조를 사용할 수 있습니다.\n",
    "    y_skeleton = keras.layers.GlobalAveragePooling1D()(skeleton_input)\n",
    "    y_hand = keras.layers.GlobalAveragePooling1D()(hand_input)    \n",
    "\n",
    "    # 모든 특징을 결합\n",
    "    combined = keras.layers.concatenate([x, y_skeleton, y_hand])\n",
    "\n",
    "    # 결합된 특징에 대한 추가 처리\n",
    "    z = keras.layers.Dense(16, activation=\"relu\")(combined)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(z)\n",
    "    \n",
    "    rnn_model = keras.Model([frame_features_input, skeleton_input, hand_input, mask_input], output)\n",
    "    rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c13067e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.8101 - accuracy: 0.0357    \n",
      "Epoch 1: val_loss improved from inf to 2.85140, saving model to ./tmp\\video_classifier.h5\n",
      "8/8 [==============================] - 18s 461ms/step - loss: 2.8177 - accuracy: 0.0333 - val_loss: 2.8514 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.7107 - accuracy: 0.0667  \n",
      "Epoch 2: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 2.7107 - accuracy: 0.0667 - val_loss: 2.9366 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6927 - accuracy: 0.0714  \n",
      "Epoch 3: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.6888 - accuracy: 0.0833 - val_loss: 2.9340 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6894 - accuracy: 0.0893  \n",
      "Epoch 4: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6882 - accuracy: 0.0833 - val_loss: 2.9164 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6695 - accuracy: 0.0893\n",
      "Epoch 5: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.6813 - accuracy: 0.0833 - val_loss: 2.9646 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6710 - accuracy: 0.0893\n",
      "Epoch 6: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6736 - accuracy: 0.0833 - val_loss: 2.9641 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6653 - accuracy: 0.0893\n",
      "Epoch 7: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6722 - accuracy: 0.0833 - val_loss: 2.9931 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 2.6534 - accuracy: 0.0833  \n",
      "Epoch 8: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6698 - accuracy: 0.0833 - val_loss: 3.0192 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6625 - accuracy: 0.0893  \n",
      "Epoch 9: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6630 - accuracy: 0.0833 - val_loss: 3.0296 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6631 - accuracy: 0.0893\n",
      "Epoch 10: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.6568 - accuracy: 0.0833 - val_loss: 3.0882 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6526 - accuracy: 0.0893  \n",
      "Epoch 11: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6582 - accuracy: 0.0833 - val_loss: 3.1539 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6550 - accuracy: 0.0714\n",
      "Epoch 12: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.6504 - accuracy: 0.0833 - val_loss: 3.1756 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6394 - accuracy: 0.0893\n",
      "Epoch 13: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6445 - accuracy: 0.0833 - val_loss: 3.2184 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6363 - accuracy: 0.0893\n",
      "Epoch 14: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.6421 - accuracy: 0.0833 - val_loss: 3.2451 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6487 - accuracy: 0.0714\n",
      "Epoch 15: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6473 - accuracy: 0.0833 - val_loss: 3.2234 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6251 - accuracy: 0.0893  \n",
      "Epoch 16: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6322 - accuracy: 0.0833 - val_loss: 3.2752 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6390 - accuracy: 0.0714  \n",
      "Epoch 17: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6277 - accuracy: 0.0833 - val_loss: 3.3102 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6324 - accuracy: 0.0893\n",
      "Epoch 18: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 2.6292 - accuracy: 0.0833 - val_loss: 3.3501 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6180 - accuracy: 0.0714\n",
      "Epoch 19: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.6212 - accuracy: 0.0833 - val_loss: 3.4262 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6366 - accuracy: 0.0714  \n",
      "Epoch 20: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.6190 - accuracy: 0.0667 - val_loss: 3.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.6230 - accuracy: 0.0536  \n",
      "Epoch 21: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6109 - accuracy: 0.0833 - val_loss: 3.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5965 - accuracy: 0.0893  \n",
      "Epoch 22: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6055 - accuracy: 0.0833 - val_loss: 3.5900 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5963 - accuracy: 0.0893\n",
      "Epoch 23: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.6035 - accuracy: 0.0833 - val_loss: 3.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5981 - accuracy: 0.0893\n",
      "Epoch 24: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.5967 - accuracy: 0.0833 - val_loss: 3.5705 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5990 - accuracy: 0.0893  \n",
      "Epoch 25: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 2.5978 - accuracy: 0.0833 - val_loss: 3.5418 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5868 - accuracy: 0.0893\n",
      "Epoch 26: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5866 - accuracy: 0.1000 - val_loss: 3.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5938 - accuracy: 0.0893  \n",
      "Epoch 27: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5820 - accuracy: 0.1000 - val_loss: 3.7606 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 2.5928 - accuracy: 0.1042\n",
      "Epoch 28: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 2.5802 - accuracy: 0.1000 - val_loss: 3.8773 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5838 - accuracy: 0.1071\n",
      "Epoch 29: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 2.5770 - accuracy: 0.1000 - val_loss: 3.9359 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.5707 - accuracy: 0.1071  \n",
      "Epoch 30: val_loss did not improve from 2.85140\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 2.5701 - accuracy: 0.1000 - val_loss: 3.9380 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     seq_model\u001b[38;5;241m.\u001b[39msave(filepath)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history, seq_model\n\u001b[1;32m---> 36\u001b[0m _, sequence_model \u001b[38;5;241m=\u001b[39m run_experiment()\n",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m seq_model \u001b[38;5;241m=\u001b[39m get_sequence_model()\n\u001b[0;32m     10\u001b[0m history \u001b[38;5;241m=\u001b[39m seq_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     11\u001b[0m     [train_data[\u001b[38;5;241m0\u001b[39m], train_data[\u001b[38;5;241m1\u001b[39m], train_data[\u001b[38;5;241m2\u001b[39m], train_data[\u001b[38;5;241m3\u001b[39m]],  \u001b[38;5;66;03m# 수정된 입력 데이터\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     train_labels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint],\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m seq_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(filepath)\n\u001b[0;32m     21\u001b[0m _, accuracy \u001b[38;5;241m=\u001b[39m seq_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m     22\u001b[0m     [test_data[\u001b[38;5;241m0\u001b[39m], test_data[\u001b[38;5;241m1\u001b[39m], test_data[\u001b[38;5;241m2\u001b[39m], test_data[\u001b[38;5;241m3\u001b[39m]],  \u001b[38;5;66;03m# 수정된 입력 데이터\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     test_labels\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mIsDirectory(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE=8\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier.h5\"\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=False, save_best_only=True, verbose=1)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1], train_data[2], train_data[3]],  # 수정된 입력 데이터\n",
    "        train_labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model = keras.models.load_model(filepath)\n",
    "    \n",
    "    _, accuracy = seq_model.evaluate(\n",
    "        [test_data[0], test_data[1], test_data[2], test_data[3]],  # 수정된 입력 데이터\n",
    "        test_labels\n",
    "    )\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    # 손실 및 정확도 그래프 출력\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    seq_model.save(filepath)\n",
    "    \n",
    "    return history, seq_model\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames, skeletons, hands):\n",
    "    num_frames = frames.shape[1]\n",
    "    video_length = min(MAX_SEQ_LENGTH, num_frames)\n",
    "\n",
    "    frame_mask = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "    frame_skeletons = np.zeros((1, MAX_SEQ_LENGTH, SKELETON_FEATURES), dtype=\"float32\")\n",
    "    frame_hands = np.zeros((1, MAX_SEQ_LENGTH, HAND_FEATURES), dtype=\"float32\")\n",
    "    frame_images = np.zeros((1, MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=\"float32\")\n",
    "    \n",
    "    feature_extractor = Sequential([\n",
    "        ResNet50(include_top=False, weights='imagenet', pooling='avg'),  # 예시로 평균 풀링 사용\n",
    "        Dense(NUM_FEATURES, activation='relu')  # NUM_FEATURES에 맞는 덴스 레이어 추가\n",
    "    ])\n",
    "    \n",
    "    for j in range(video_length):\n",
    "        # 이미지 데이터 전처리 및 특징 추출\n",
    "        image_feature = preprocess_image(frames[j:j+1])\n",
    "        image_feature = np.expand_dims(image_feature, axis=0)\n",
    "        feature_result = feature_extractor.predict(image_feature)\n",
    "\n",
    "        # frame_images 대신 feature_result를 사용\n",
    "        frame_features[0, j] = feature_result\n",
    "\n",
    "        # Mediapipe 데이터 전처리\n",
    "        skeleton_feature = preprocess_skeleton_data(skeletons[j])\n",
    "        hand_feature = preprocess_hand_data(hands[j])\n",
    "\n",
    "        frame_skeletons[0, j] = skeleton_feature\n",
    "        frame_hands[0, j] = hand_feature\n",
    "        frame_mask[0, j] = 1\n",
    "\n",
    "    return frame_features, frame_skeletons, frame_hands, frame_mask, frame_images\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    frames, skeletons, hands = load_video(path)    \n",
    "    try:\n",
    "        num_frames = frames.shape[1]\n",
    "    except IndexError:\n",
    "        print(\"Error: Unable to determine the number of frames. Frames shape:\", frames.shape)\n",
    "        return None\n",
    "    \n",
    "    frame_features, frame_skeletons, frame_hands, frame_mask, frame_images = prepare_single_video(frames, skeletons, hands)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_skeletons, frame_hands, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"{class_vocab[i]} : {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    return frame_images\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0aabe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
