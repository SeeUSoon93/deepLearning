{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275440b6",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "- 컴퓨터비전, 머신러닝관련하여 다양한 알고리즘을 제공\n",
    "- OpenCV 데이터가 내부적으로 Numpy 배열로 변환되어 내부 처리를 수행\n",
    "- Numpy, Matplotlib과 호환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210323e",
   "metadata": {},
   "source": [
    "### 이미지 읽고 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe45c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66f36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손가락만\n",
    "# 라이브러리 불러오기\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 미디어 파이프에서 제공하는 드로잉 유틸리티와 손 모델을 사용하기 위한 인스턴스를 생성\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.\n",
    "# 웹캠을 사용하기 위해 cv2.VideoCapture 객체를 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 손을 감지하기 위한 설정으로 'Hands' 객체를 생성\n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=2,  # 최대 감지할 손의 개수\n",
    "    min_detection_confidence=0.5,  # 감지를 위한 최소 신뢰도\n",
    "    min_tracking_confidence=0.5) as hands:  # 추적을 위한 최소 신뢰도\n",
    "\n",
    "    # 웹캠이 열려 있는 동안 무한 루프를 돌면서 프레임을 읽기\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue  # 읽기에 실패하면 다음 프레임으로 건너뛰기\n",
    "\n",
    "        # 이미지를 좌우반전시키고 RGB로 변환(미디어 파이프가 RGB 이미지를 사용)\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 변환된 이미지로 손을 감지\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # 다시 BGR로 이미지를 변환하여 OpenCV에서 사용\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 감지된 손의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # 랜드마크 좌표를 가져와서 특정 손가락의 x 좌표를 계산\n",
    "                finger1 = int(hand_landmarks.landmark[4].x * 100)\n",
    "                finger2 = int(hand_landmarks.landmark[8].x * 100)\n",
    "                # 손가락들 사이의 거리를 계산\n",
    "                dist = abs(finger1 - finger2)\n",
    "                # 계산된 정보를 이미지 위에 텍스트로 표시\n",
    "                cv2.putText(\n",
    "                    image, text='f1=%d f2=%d dist=%d ' % (finger1, finger2, dist), org=(10, 30),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1,\n",
    "                    color=(255, 255, 255), thickness=3)\n",
    "\n",
    "                # 손의 랜드마크와 연결선을 이미지 위에 그리기\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # 처리된 이미지를 'image'라는 창에 표시\n",
    "        cv2.imshow('image', image)\n",
    "        # 'q' 키를 누르면 루프에서 빠져나와 프로그램을 종료\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "# 사용이 끝난 후, 웹캠을 해제\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2681d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mflip(image, \u001b[38;5;241m1\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 변환된 이미지로 손을 감지\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m hands_results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 변환된 이미지로 포즈를 감지\u001b[39;00m\n\u001b[0;32m     34\u001b[0m pose_results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 손가락-포즈\n",
    "# 라이브러리 불러오기\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 미디어 파이프에서 제공하는 드로잉 유틸리티와 모델을 사용하기 위한 인스턴스를 생성\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# 웹캠을 사용하기 위해 cv2.VideoCapture 객체를 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Hands와 Pose 객체를 생성\n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands, mp_pose.Pose(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    # 웹캠이 열려 있는 동안 무한 루프를 돌면서 프레임을 읽기\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue  # 읽기에 실패하면 다음 프레임으로 건너뛰기\n",
    "\n",
    "        # 이미지를 좌우반전시키고 RGB로 변환(미디어 파이프가 RGB 이미지를 사용)\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 변환된 이미지로 손을 감지\n",
    "        hands_results = hands.process(image)\n",
    "        \n",
    "        # 변환된 이미지로 포즈를 감지\n",
    "        pose_results = pose.process(image)\n",
    "\n",
    "        # 다시 BGR로 이미지를 변환하여 OpenCV에서 사용\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 감지된 손의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                # 손의 랜드마크와 연결선을 이미지 위에 그리기\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # 감지된 포즈의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if pose_results.pose_landmarks:\n",
    "            # 포즈의 랜드마크와 연결선을 이미지 위에 그리기\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # 처리된 이미지를 'image'라는 창에 표시\n",
    "        cv2.imshow('image', image)\n",
    "        # 'q' 키를 누르면 루프에서 빠져나와 프로그램을 종료\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "# 사용이 끝난 후, 웹캠을 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836b27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손가락, 포즈, 얼굴\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 미디어 파이프에서 제공하는 드로잉 유틸리티와 모델을 사용하기 위한 인스턴스를 생성\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# 웹캠을 사용하기 위해 cv2.VideoCapture 객체를 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Hands, Pose, FaceMesh 객체를 생성\n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands, mp_pose.Pose(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as pose, mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "    # 웹캠이 열려 있는 동안 무한 루프를 돌면서 프레임을 읽기\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            continue  # 읽기에 실패하면 다음 프레임으로 건너뛰기\n",
    "\n",
    "        # 이미지를 좌우반전시키고 RGB로 변환(미디어 파이프가 RGB 이미지를 사용)\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 변환된 이미지로 손을 감지\n",
    "        hands_results = hands.process(image)\n",
    "        \n",
    "        # 변환된 이미지로 포즈를 감지\n",
    "        pose_results = pose.process(image)\n",
    "        \n",
    "        # 변환된 이미지로 얼굴 랜드마크를 감지\n",
    "        face_results = face_mesh.process(image)\n",
    "\n",
    "        # 다시 BGR로 이미지를 변환하여 OpenCV에서 사용\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 감지된 손의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # 감지된 포즈의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "        # 감지된 얼굴의 랜드마크가 있으면 정보를 화면에 표시\n",
    "        if face_results.multi_face_landmarks:\n",
    "            for face_landmarks in face_results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, face_landmarks,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1))\n",
    "\n",
    "        # 처리된 이미지를 'image'라는 창에 표시\n",
    "        cv2.imshow('image', image)\n",
    "        # 'q' 키를 누르면 루프에서 빠져나와 프로그램을 종료\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# 사용이 끝난 후, 웹캠을 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
