{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c60bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리들을 임포트합니다.\n",
    "from tensorflow_docs.vis import embed  # TensorFlow 문서의 시각화를 위한 도구입니다.\n",
    "from tensorflow import keras  # TensorFlow의 고수준 신경망 API입니다.\n",
    "from imutils import paths  # 이미지 처리를 위한 유틸리티 함수들을 제공합니다.\n",
    "\n",
    "import matplotlib.pyplot as plt  # 그래프와 이미지를 시각화하기 위한 라이브러리입니다.\n",
    "import tensorflow as tf  # TensorFlow 라이브러리입니다, 딥러닝 모델을 구성하고 훈련하기 위해 사용됩니다.\n",
    "import pandas as pd  # 데이터 분석 및 조작을 위한 라이브러리입니다.\n",
    "import numpy as np  # 수치 계산을 위한 라이브러리입니다.\n",
    "import imageio  # 이미지 읽기/쓰기를 위한 라이브러리입니다.\n",
    "import cv2  # OpenCV 라이브러리, 이미지 및 비디오 처리를 위해 사용됩니다.\n",
    "import os  # 운영체제와 상호작용을 위한 라이브러리입니다, 파일 경로 조작에 주로 사용됩니다.\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0718e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a437eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.listdir('dataset/train')\n",
    "label_types = os.listdir('dataset/train')\n",
    "# 훈련 데이터셋을 위한 비어있는 리스트 초기화\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/train' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/train'+'/'+item)    \n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/train'+'/'+item)+'/'+room))\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name']).loc[:,['video_name','tag']]\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('train.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b961d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "room_types = os.listdir('dataset/test')\n",
    "\n",
    "rooms = []\n",
    "# dataset_path에 저장된 각 항목(방 유형)에 대해 반복\n",
    "for item in dataset_path:\n",
    "    # 'dataset/test' 폴더 내 각 방 유형별로 모든 파일 이름을 가져옴\n",
    "    all_rooms = os.listdir('dataset/test'+'/'+item)\n",
    "    # 가져온 파일 이름을 rooms 리스트에 추가\n",
    "    for room in all_rooms:\n",
    "        rooms.append((item, str('dataset/test'+'/'+item)+'/'+room))\n",
    "\n",
    "# rooms 리스트를 사용하여 데이터프레임 생성\n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag','video_name'])\n",
    "df = train_df.loc[:,['video_name','tag']]\n",
    "# 생성된 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('test.csv', encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 크기, 배치 크기, 에포크 수를 정의하는 하이퍼파라미터입니다.\n",
    "IMG_SIZE = 224  # 입력 이미지의 크기를 정의합니다.\n",
    "BATCH_SIZE = 64  # 한 번에 처리할 이미지의 수를 정의합니다.\n",
    "EPOCHS = 10  # 모델을 훈련할 때 전체 데이터셋을 반복할 횟수를 정의합니다.\n",
    "\n",
    "# 비디오 처리에 사용할 최대 시퀀스 길이와 특징 벡터의 크기를 정의합니다.\n",
    "MAX_SEQ_LENGTH = 20  # 처리할 비디오의 최대 프레임 수를 정의합니다.\n",
    "NUM_FEATURES = 2048  # 비디오 프레임에서 추출할 특징의 차원 수를 정의합니다.\n",
    "\n",
    "\"\"\"\n",
    "## 데이터 준비 부분\n",
    "\"\"\"\n",
    "\n",
    "# 훈련 및 테스트 데이터를 포함하는 CSV 파일을 읽어서 DataFrame 객체를 생성합니다.\n",
    "train_df = pd.read_csv(\"train.csv\")  # 훈련 데이터셋을 불러옵니다.\n",
    "test_df = pd.read_csv(\"test.csv\")  # 테스트 데이터셋을 불러옵니다.\n",
    "\n",
    "# 훈련 및 테스트 데이터셋의 비디오 수를 출력합니다.\n",
    "print(f\"Total videos for training: {len(train_df)}\")  # 훈련용 비디오의 총 개수를 출력합니다.\n",
    "print(f\"Total videos for testing: {len(test_df)}\")  # 테스트용 비디오의 총 개수를 출력합니다.\n",
    "\n",
    "# 훈련 데이터셋에서 무작위로 10개의 샘플을 출력합니다.\n",
    "train_df.sample(10)  # 훈련 데이터셋의 예시를 출력하기 위해 무작위로 10개의 샘플을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 프레임의 중앙을 정사각형으로 자르는 함수입니다.\n",
    "def crop_center_square(frame):\n",
    "    # 프레임의 높이와 너비를 가져옵니다.\n",
    "    y, x = frame.shape[0:2]\n",
    "    # 높이와 너비 중 더 작은 값을 선택합니다.\n",
    "    min_dim = min(y, x)\n",
    "    # 중앙에서 시작할 x, y 좌표를 계산합니다.\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    # 정사각형 프레임을 반환합니다.\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "# 비디오를 로드하고, 선택적으로 프레임의 수를 제한하며, 프레임을 재조정하는 함수입니다.\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    # 비디오 경로로부터 비디오 캡처 객체를 생성합니다.\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []  # 프레임을 저장할 리스트입니다.\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # 프레임을 하나씩 읽습니다.\n",
    "            if not ret:\n",
    "                break  # 더 이상 프레임이 없으면 종료합니다.\n",
    "            frame = crop_center_square(frame)  # 중앙 정사각형으로 자릅니다.\n",
    "            frame = cv2.resize(frame, resize)  # 프레임을 재조정합니다.\n",
    "            frame = frame[:, :, [2, 1, 0]]  # BGR을 RGB로 순서를 변경합니다.\n",
    "            frames.append(frame)  # 프레임을 리스트에 추가합니다.\n",
    "\n",
    "            if len(frames) == max_frames:  # 최대 프레임 수에 도달하면 종료합니다.\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()  # 비디오 캡처 객체를 해제합니다.\n",
    "    return np.array(frames)  # 프레임의 배열을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deded2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 추출기를 구축하는 함수입니다.\n",
    "def build_feature_extractor():\n",
    "    # InceptionV3 모델을 특징 추출기로 사용합니다. ImageNet 데이터로 사전 훈련된 가중치를 사용하고, \n",
    "    # 최상위 층은 포함하지 않으며, 평균 풀링을 사용합니다.\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    # InceptionV3에 맞게 입력 데이터를 전처리하는 함수입니다.\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # 모델의 입력을 정의합니다.\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    # 입력 데이터를 전처리합니다.\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    # 전처리된 데이터를 특징 추출기에 통과시켜 출력을 얻습니다.\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    # 입력과 출력을 연결하는 케라스 모델을 생성합니다.\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "# 특징 추출기 모델을 생성합니다.\n",
    "feature_extractor = build_feature_extractor()\n",
    "\n",
    "# 클래스 라벨을 정수로 변환하는 레이어를 생성합니다.\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "# 라벨 프로세서가 이해하는 어휘목록을 출력합니다.\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터프레임(df)과 루트 디렉터리(root_dir)를 사용하여 모든 비디오를 준비하는 함수입니다.\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)  # 샘플의 수를 결정합니다.\n",
    "    video_paths = df[\"video_name\"].values.tolist()  # 비디오 경로를 리스트로 변환합니다.\n",
    "    labels = df[\"tag\"].values  # 라벨 값을 가져옵니다.\n",
    "    labels = label_processor(labels[..., None]).numpy()  # 라벨을 처리하여 넘파이 배열로 변환합니다.\n",
    "\n",
    "    # `frame_masks`와 `frame_features`는 우리의 순차 모델에 제공될 데이터입니다.\n",
    "    # `frame_masks`는 시간 단계가 패딩으로 마스킹되었는지 여부를 나타내는 부울 값들을 포함합니다.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # 각 비디오에 대해서.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # 모든 프레임을 수집하고 배치 차원을 추가합니다.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # 현재 비디오의 마스크와 특징을 저장하기 위한 임시 공간을 초기화합니다.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # 현재 비디오의 프레임에서 특징을 추출합니다.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                # 특징 추출기를 사용하여 각 프레임의 특징을 예측합니다.\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            # 마스크를 설정합니다. 1 = 마스킹되지 않음, 0 = 마스킹됨\n",
    "            temp_frame_mask[i, :length] = 1  \n",
    "\n",
    "        # 임시 특징과 마스크를 각각의 특징 및 마스크 배열에 할당합니다.\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    # 특징과 마스크 배열, 라벨을 반환합니다.\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "# 훈련 데이터와 라벨을 준비합니다.\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "# 테스트 데이터와 라벨을 준비합니다.\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "# 훈련 데이터 세트의 프레임 특징과 마스크의 차원을 출력합니다.\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ad767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순차 모델을 위한 유틸리티입니다.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    # 입력으로 프레임 특징과 마스크를 받습니다.\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # `mask` 사용의 중요성을 이해하기 위한 참고 자료:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    # 출력 계층은 분류할 클래스 수만큼의 노드를 가집니다.\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# 실험을 실행하기 위한 유틸리티입니다.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ceed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 비디오를 준비하기 위한 함수입니다.\n",
    "def prepare_single_video(frames):\n",
    "    # ... (중략) ...\n",
    "\n",
    "# 비디오의 시퀀스 예측을 수행하는 함수입니다.\n",
    "def sequence_prediction(path):\n",
    "    # ... (중략) ...\n",
    "\n",
    "# 이 유틸리티는 시각화를 위한 것입니다.\n",
    "# 참조된 자료:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    # ... (중략) ...\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "to_gif(test_frames[:MAX_SEQ_LENGTH])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
