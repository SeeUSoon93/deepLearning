{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51608630",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m letterbox\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m non_max_suppression_kpt\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m output_to_keypoint, plot_skeleton_kpts\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6b76a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/13/24/23cdf7e7dc33e5c01588c315f8424d31afa9edb05a80168f3d44f7178ff7/torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (2023.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\seeusoon\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.16.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.6/1.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 14.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fcaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_video(frame):\n",
    "    # 현재 프레임을 복사합니다.\n",
    "    mapped_img = frame.copy()\n",
    "    # 프레임을 입력 사이즈에 맞게 재조정합니다.\n",
    "    img = letterbox(frame, input_size, stride=64, auto=True)[0]\n",
    "    print(img.shape)\n",
    "    img_ = img.copy()\n",
    "    # 배열을 4D 텐서로 변환합니다.\n",
    "    img = transforms.ToTensor()(img)\n",
    "    # 텐서로 변환된 배열을 다시 NumPy 배열로 변환합니다.\n",
    "    img = torch.tensor(np.array([img.numpy()]))\n",
    "    # 이미지를 계산 장치(CPU 또는 GPU)로 로드합니다.\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # 추론 시에는 그래디언트를 저장하지 않습니다.\n",
    "    with torch.no_grad():\n",
    "        t1 = time.time()\n",
    "        output, _ = model(img)\n",
    "        t2 = time.time()\n",
    "        fps = 1/(t2 - t1)\n",
    "        # 비최대 억제를 사용하여 출력을 처리합니다.\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=1, nkpt=17, kpt_label=True)\n",
    "        # 출력을 키포인트로 변환합니다.\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    # 이미지 형식을 [b, c, h, w]에서 [h, w, c]로 변경하여 화면에 표시합니다.\n",
    "    nimg = img[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 출력된 각 객체에 대한 포즈를 그립니다.\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "        \n",
    "    return nimg, fps\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "\n",
    "# 포워드 패스의 입력 크기를 설정합니다.\n",
    "input_size = 256\n",
    "\n",
    "#---------------------------초기화---------------------------------------------#\n",
    "\n",
    "# 하드웨어 구성에 따라 장치를 선택합니다.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Selected Device : ', device)\n",
    "\n",
    "# 키포인트 감지 모델을 로드합니다.\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model']\n",
    "# 모델을 평가 모드로 설정합니다.\n",
    "_ = model.float().eval()\n",
    "# 모델을 계산 장치에 로드합니다.\n",
    "model.to(device)\n",
    "\n",
    "# 비디오 캡쳐와 라이터를 초기화합니다.\n",
    "videos = ['dance', 'dark', 'far-away', 'occlusion-example', 'skydiving', 'yoga-1']\n",
    "\n",
    "file_name = videos[0] + '.mp4'\n",
    "vid_path = '../Media/' + file_name\n",
    "\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "ret, frame = cap.read()\n",
    "h, w, _ = frame.shape\n",
    "\n",
    "# letterbox 함수가 이미지 크기를 변경하므로 w, h를 변경해야 할 수 있습니다.\n",
    "out = cv2.VideoWriter('pose_outputs/' + file_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print('Unable to read frame. Exiting ..')\n",
    "            break\n",
    "\n",
    "        img, fps_ = pose_video(frame)\n",
    "\n",
    "        # FPS와 모델명을 화면에 표시합니다.\n",
    "        cv2.putText(img, 'FPS : {:.2f}'.format(fps_), (200, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, 'YOLOv7', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 결과를 화면에 표시하고 저장합니다.\n",
    "        cv2.imshow('Output', img[...,::-1])\n",
    "        out.write(img[...,::-1])\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "        \tbreak\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
